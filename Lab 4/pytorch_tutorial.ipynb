{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([       nan, 1.8623e-42])\n"
     ]
    }
   ],
   "source": [
    "# tensor filled with uninitialized data\n",
    "em = torch.empty(2)\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "em = torch.empty(1,2)\n",
    "print(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "# Initializing Tensor Directly from data\n",
    "x = torch.tensor([2.5, .1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2214, 0.6021, 0.6976],\n",
      "        [0.8427, 0.5447, 0.6803],\n",
      "        [0.1191, 0.8889, 0.4112],\n",
      "        [0.9020, 0.9231, 0.1585],\n",
      "        [0.0677, 0.9606, 0.7129]])\n",
      "tensor([0.2214, 0.8427, 0.1191, 0.9020, 0.0677])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:, 0])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58,  64],\n",
       "        [139, 154]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "c = torch.matmul(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([1., 3.]),\n",
      "indices=tensor([0, 0])) tensor(4.) tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor = torch.Tensor([[1, 2],\n",
    "                       [3, 4]])\n",
    "\n",
    "# get minmum value in each row\n",
    "tensor_min = torch.min(tensor, dim=1)\n",
    "# get max value in tensor\n",
    "tensor_max = torch.max(tensor)\n",
    "# calculate mean value\n",
    "tensor_mean = torch.mean(tensor)\n",
    "...  # Every single mathematical function you could imagine.\n",
    "\n",
    "print(tensor_min, tensor_max, tensor_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.]) tensor([1.5000, 3.5000, 5.5000])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]])  # shape = (3, 2)\n",
    "\n",
    "tensor_mean_row = torch.mean(tensor, dim=0)  # shape = (2,) Averaging over 1st dimension (along columns)\n",
    "tensor_mean_col = torch.mean(tensor, dim=1)  # shape = (3,) Averaging over 2nd dimension (along rows)\n",
    "\n",
    "print(tensor_mean_row, tensor_mean_col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!![image.png](https://i.stack.imgur.com/ORqaP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1627, 0.1329, 0.0312, 0.3476],\n",
      "        [0.5599, 0.7680, 0.3398, 0.5478],\n",
      "        [0.9847, 0.0399, 0.6875, 0.2004],\n",
      "        [0.6235, 0.6870, 0.6036, 0.2973]])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "# view function is a view of the tensor, it does not create a new tensor\n",
    "# but modifies the original tensor and changes the shape\n",
    "y = x.view(-1,4 )\n",
    "\n",
    "z = x.view(1,16 )\n",
    "print(y.size())\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "type of b <class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "type of c <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b= a.numpy()\n",
    "print(b)\n",
    "print(f\"type of b {type(b)}\")\n",
    "c = torch.from_numpy(b)\n",
    "print(c)\n",
    "print(f\"type of c {type(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(5)\n",
    "    # By default, tensors are created on the CPU. We need to explicitly move tensors \n",
    "    # to the GPU using .to method (after checking for GPU availability)\n",
    "    y = y.to(device)\n",
    "    z = x*y\n",
    "    z=z.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The general framework for training your model is as follows\n",
    "for epoch in range(num_epochs):\n",
    "\tfor i in range(num_batches):\n",
    "\t\tForward pass\n",
    "\t\tLoss calculation\n",
    "\t\tBackward pass\n",
    "\t\tWeights update\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines arrays in a vertically stacked sequence (used for data manipulation)\n",
    "from numpy import vstack\n",
    "\n",
    "# Reads a CSV file into a DataFrame (used for loading datasets)\n",
    "from pandas import read_csv\n",
    "\n",
    "# Encodes categorical labels into numerical format (used for label preprocessing)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Calculates the accuracy of a classification model (used for model evaluation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defines a custom dataset class for PyTorch (used for handling data)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Creates a DataLoader for efficient batch processing in PyTorch (used for data loading)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Splits a dataset into training and validation sets (used for data splitting)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Represents a multi-dimensional matrix in PyTorch (used for tensor manipulation)\n",
    "from torch import Tensor\n",
    "\n",
    "# Implements a linear layer in a neural network (used for defining neural network architecture)\n",
    "from torch.nn import Linear\n",
    "\n",
    "# Applies rectified linear unit (ReLU) activation function (used for introducing non-linearity)\n",
    "from torch.nn import ReLU\n",
    "\n",
    "# Applies sigmoid activation function (used for binary classification output)\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "# Base class for all neural network modules in PyTorch (used for creating custom models)\n",
    "from torch.nn import Module\n",
    "\n",
    "# Stochastic Gradient Descent optimizer (used for model optimization during training)\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Binary Cross Entropy Loss function (used for binary classification problems)\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "# Initializes weights using Kaiming uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "\n",
    "# Initializes weights using Xavier (Glorot) uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import xavier_uniform_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "PyTorch provides two data primitives: ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``\n",
    "that allow you to use pre-loaded datasets as well as your own data.\n",
    "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
    "the ``Dataset`` to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset definition\n",
    "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    # The __init__ function is run once when instantiating the Dataset object\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        original_labels = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(original_labels)\n",
    "        \n",
    "        # this dictionary \n",
    "        self.encoding_mapping = {encoded_label: original_label for encoded_label, original_label  in zip(self.y, original_labels)}\n",
    "\n",
    "        # ensure the target is float\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    # The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    # The Dataset retrieves our dataset’s features and labels one sample at a time.\n",
    "    # While training a model, we typically want to pass samples in “minibatches”,\n",
    "    # reshuffle the data at every epoch to reduce model overfitting,\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return dataset.encoding_mapping, train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        # initializes the weights of the self.hidden1 layer using the Kaiming (He) Uniform initialization method\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        # initializes the weights of the self.hidden3 layer using the Xavier (Glorot) Uniform initialization method.\n",
    "        #it is commonly used for layers with hyperbolic tangent (tanh) or sigmoid activation functions.\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss() # Binary Cross-Entropy\n",
    "    # Stochastic Gradient Descent Optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model: Module):\n",
    "    # convert row to data\n",
    "    model.eval() # This is equivalent with self.train(False)\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001DD55FD04C0>\n",
      "235 116\n",
      "Accuracy: 0.905\n",
      "Predicted: 1.000 (class=1) which is (class=g)\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "# dataset link https://datahub.io/machine-learning/ionosphere\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "encoding_mapping, train_dl, test_dl = prepare_data(path)\n",
    "print(train_dl)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(34)\n",
    "\n",
    "model.train(True) # set it to False for inference \n",
    "\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "\n",
    "print('Predicted: %.3f (class=%d) which is (class=%s)' % (yhat, yhat.round(), encoding_mapping[int(yhat.round())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
