{"cells":[{"cell_type":"code","execution_count":32,"id":"291c7c1d","metadata":{"id":"291c7c1d"},"outputs":[],"source":["# Student one full name\n","Student1_Name = \"Zeyad Tarek\"\n","\n","# Student two full name\n","Student2_Name = \"Asmaa Adel\"\n","\n","# team ID\n","team_ID = \"30\""]},{"cell_type":"code","execution_count":33,"id":"9818bc3c","metadata":{"id":"9818bc3c"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk import bigrams, FreqDist"]},{"cell_type":"markdown","id":"fa3f0456","metadata":{"id":"fa3f0456"},"source":["# Requirement Description\n","You are given pizza orders dataset. You are required to build some Language Models from the training set. You will then use these models for predicting the masked tokens in the test set. For each order in the test set, a random word was replaced with the word mask. Your task will be to predict the mask work. This requirement is divided into three parts:\n","\n","## Part 1: Data Preprocessing\n","In this part, you will preprocess the given dataset. Once you have preprocessed the orders, you can then use them to build the language models.\n","\n","## Part 2: Bi-gram Language Model\n","In this part, You will build a bi-gram language model from scratch. The Bi-gram language model need to have two main functions: train, predict.\n","\n","## Part 3: RNN Language Model\n","In this part, You will build an RNN language model from scratch. The RNN language model need to have two main functions: train, predict.\n","\n","---\n","Let's get started :D"]},{"cell_type":"code","execution_count":34,"id":"b0a2d337","metadata":{"id":"b0a2d337"},"outputs":[{"name":"stdout","output_type":"stream","text":["train set size = 10000\n","test set size = 1000\n"]}],"source":["# Read the training and test sets\n","train_sentences = pd.read_csv('pizza_train.csv')['text'].to_list()\n","test_sentences = pd.read_csv('masked_pizza_test.csv')['text'].to_list()\n","print(f\"train set size = {len(train_sentences)}\")\n","print(f\"test set size = {len(test_sentences)}\")"]},{"cell_type":"markdown","id":"83a2cf4c","metadata":{"id":"83a2cf4c"},"source":["# Part 1: Data Preproccessing\n","After reading the train and test sets You will do the following steps on both train and test sets:\n","Loop over the sentences to do the following\n","   1. convert all characters to lower (hint: lower())\n","   2. tokenize the sentence (hint: use word_tokenize())\n","   3. Add the start sentence \\<s> and end sentence \\</s> tokens at the beginning and end of each tokenized sentence\n","   4. Add the tokens of the sentence in a predefined set to collect the vocabulary"]},{"cell_type":"code","execution_count":35,"id":"d6148c80","metadata":{"id":"d6148c80"},"outputs":[],"source":["# This function takes a List of sentences to preprocess them and vocabulary to extend\n","# It returns a list of sentences after preprocessing and the vocabulary it found\n","def preprocess(sentences, vocab):\n","    tokenized_sentences = []\n","    for i in range(len(sentences)):\n","        sentence = sentences[i]\n","\n","        ################################ TODO: Preprocessing  ####################################\n","        # convert all characters to lower (hint: use lower())\n","        sentence = sentence.lower()\n","\n","        # tokenize the sentence (hint: use word_tokenize())\n","        tokens = word_tokenize(sentence)\n","\n","        # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n","        tokens = ['<s>'] + tokens + ['</s>']\n","\n","        # Add the tokens of the sentence in the predefined set vocab to collect the vocabulary\n","        vocab.update(tokens)\n","        \n","        # Add the sentence to the tokenized_sentences\n","        tokenized_sentences.append(tokens)\n","\n","        #########################################################################################\n","\n","    return tokenized_sentences, vocab"]},{"cell_type":"code","execution_count":36,"id":"1d0f0f04","metadata":{"id":"1d0f0f04"},"outputs":[],"source":["vocab = set()\n","preprocessed_train_sentences, vocab = preprocess(train_sentences, vocab)\n","preprocessed_test_sentences, vocab = preprocess(test_sentences, vocab)\n","vocab.remove(\"mask\")\n","\n","\n","assert type(preprocessed_train_sentences) == list, \"Type of preprocessed train should be list\"\n","assert type(preprocessed_test_sentences) == list, \"Type of preprocessed test should be list\"\n","assert len(preprocessed_train_sentences) == 10000, \"The number of train sentences is not correct\"\n","assert len(preprocessed_test_sentences) == 1000, \"The number of train sentences is not correct\"\n","assert len(vocab) == 307, \"Error in the number of vocabulary\""]},{"cell_type":"markdown","id":"48f2a175","metadata":{"id":"48f2a175"},"source":["# Part two:\n","In this part, You will build an Add one (La place) Smoothed bi-gram language model"]},{"cell_type":"code","execution_count":37,"id":"6e0664c6","metadata":{"id":"6e0664c6"},"outputs":[],"source":["class BigramLM:\n","    \n","    # The class constructor takes the vocabulary\n","    # creates a |V| * |V| numpy 2D matrix for the bi-gram model filled with zeros\n","    # It also creates two dictionaries to be used for token identification\n","    def __init__(self, vocab):\n","        self.id2word = {i: word for i, word in enumerate(list(vocab))}\n","        self.word2id = {word: i for i, word in self.id2word.items()}\n","        self.vocab_size = len(vocab)\n","    \n","        # create a numpy |V| * |V| 2D array filled with zeros\n","        self.CountsMatrix = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n","        \n","    \n","    ####################################### TODO: Complete the BigramLM class ######################################\n","    # This function is responsible for training the Language Model\n","    # It is given the preprocessed training set sentences\n","    # The goal is to fill the 2D matrix with the appropriate counts\n","    # hint: check bigrams() and FreqDist() from nltk\n","    # hint: loop over the sentences to fill the matrix with the appropriate counts\n","    def train(self, train_sentences):\n","        sentenceBigrams = []\n","        for sentence in train_sentences:\n","            sentenceBigrams += list(bigrams(sentence))\n","        freqDist = FreqDist(bigram for bigram in sentenceBigrams)\n","        for bigram in set(sentenceBigrams):\n","            self.CountsMatrix[self.word2id[bigram[0]]][self.word2id[bigram[1]]] = freqDist[bigram]\n","           \n","                \n","    \n","    # this function takes two words and calculates the Add-one Smoothed bi-gram probability of it\n","    # Of course the function will make use of the 2D counts matrix built while training\n","    # The function assumes that word1 precedes word2\n","    # The function must return the calculated probability\n","    def calcProbability(self, word1, word2):\n","        # Add-one smoothing formula for bigram probability\n","        numerator = self.CountsMatrix[self.word2id[word1]][self.word2id[word2]] + 1\n","        denominator = sum(self.CountsMatrix[self.word2id[word1]]) + self.vocab_size\n","        return numerator / denominator\n","    \n","    \n","    # This function takes a preprocessed tokenized sentence with exactly one token = mask (look at the masked test set)\n","    # The function returns a word from the vocabulary that is more likely to be masked\n","    # hint: calculate the probabilities of all possible words in the vocabulary and return the word with the maximum probability\n","    def predict(self, tokenized_sentence):\n","        maskIndex = tokenized_sentence.index('mask') - 1 # get the index of the 'mask' token\n","        maxFreqIdx = np.argmax(self.CountsMatrix[self.word2id[tokenized_sentence[maskIndex]]]) # get the index of the word with the maximum frequency\n","        return self.id2word[maxFreqIdx]\n","    \n","    \n","    # This function takes a token and samples the next token\n","    # hint: check np.random.choice\n","    def sample(self, token):\n","        nextTokenCounts = self.CountsMatrix[self.word2id[token]] # All the words in the vocab, represented by their count after the given token\n","        tokenCountsNormalized = (nextTokenCounts + 1) / float(sum(nextTokenCounts) + self.vocab_size) # Add-one Smoothed probability distribution of the next token\n","        nextTokenIdx = np.random.choice(self.vocab_size, p=tokenCountsNormalized)\n","        return self.id2word[nextTokenIdx]\n","    \n","    \n","    # This function generates a new order using the Bi-gram probabilities\n","    # Returns one string where the generated tokens are white space separated\n","    # Note: The start token <s> and end token </s> shouldn't appear in the generated order\n","    # hint: start the generation with <s> and stop generating tokens when you reach </s>\n","    def generateOrder(self):\n","        nextToken = self.sample('<s>')\n","        generatedSentence = ''\n","        while nextToken != '</s>':\n","            while nextToken == '<s>': nextToken = self.sample(nextToken)\n","            generatedSentence += nextToken + ' '\n","            nextToken = self.sample(nextToken)\n","        return generatedSentence\n","    ################################################################################################################"]},{"cell_type":"markdown","id":"be7b4632","metadata":{"id":"be7b4632"},"source":["# The next cell to make sure your Bigram LM is correct"]},{"cell_type":"code","execution_count":38,"id":"e9cba284","metadata":{"id":"e9cba284"},"outputs":[],"source":["bigramLM = BigramLM(vocab)\n","bigramLM.train(preprocessed_train_sentences)\n","\n","assert bigramLM.calcProbability('i', \"'d\") == 0.6489202480222365, \"Probability Error\"\n","assert bigramLM.calcProbability(\"'d\", \"i\") == 0.000299311583358276, \"Probability Error\"\n","assert bigramLM.predict(['<s>', 'one', 'mask', 'sized', 'green', 'pepper', '</s>']) == 'party', \"Prediction Error\""]},{"cell_type":"markdown","id":"0e67454a","metadata":{"id":"0e67454a"},"source":["# Part three:\n","In this part, you will build and RNN Language model from scratch. This part may be a little bit tricky specially in the back propagation, but we will go trough it step by step"]},{"cell_type":"code","execution_count":39,"id":"d5ead671","metadata":{"id":"d5ead671"},"outputs":[],"source":["class RNNLM:\n","    \n","    # The class constructor takes the vocabulary\n","    # creates two dictionaries to be used for token identification\n","    # creates the embeddings matrix (one hot encodings for tokens)\n","    # creates all wights of the RNN Wx, Wh, b, Wy, by\n","    def __init__(self, vocab, hidden_dim=128):\n","        vocab.add('null')\n","        self.id2word = {i: word for i, word in enumerate(list(vocab))}\n","        self.word2id = {word: i for i, word in self.id2word.items()}\n","        self.vocab_size = len(vocab)\n","        self.null_word_index = self.word2id['null']\n","        \n","        # Create a numpy |V| * |V| 2D identity array\n","        self.embeddings = np.identity(self.vocab_size)\n","        \n","        # Initialize RNN weights\n","        np.random.seed(5)\n","        self.Wx = np.random.randn(self.vocab_size, hidden_dim).astype(np.float64)\n","        self.Wx /= np.sqrt(self.vocab_size)\n","        self.Wh = np.random.randn(hidden_dim, hidden_dim).astype(np.float64)\n","        self.Wh /= np.sqrt(hidden_dim)\n","        self.b = np.zeros(hidden_dim).astype(np.float64)\n","        self.Wy = np.random.randn(hidden_dim, self.vocab_size).astype(np.float64)\n","        self.Wy /= np.sqrt(hidden_dim)\n","        self.by = np.zeros(self.vocab_size).astype(np.float64)\n","\n","        \n","    def one_step_forward(self, x, h_prev, Wx, Wh, b):\n","        \"\"\"\n","        This function runs one time step of the RNN. It should implement the RNN equation\n","        that takes input embedding and the previous hidden state then produces the new hidden state.\n","        \n","        Inputs:\n","        - x: Input data of the current time step of shape (N, D) \n","            where N is the number of tokens and D is the vocab size\n","        - h_prev: the hidden state from the previous time step of shape (N, H)\n","        - Wx: the weight matrix for input to hidden transformation of shape (D, H)\n","        - Wh: the weight matrix for hidden to hidden tranformation of shape (H, H)\n","        - b: the bias of shape (H,)\n","        \n","        Returns a tuple of:\n","        - next_h: Next hidden state of shape (N, H)\n","        - cache: a tuple of all data needed for backpropagation\n","        \"\"\"\n","        next_h, cache = None, None\n","        \n","        ############################### TODO: Implement RNN equation for forward pass ##############################\n","        \n","        # Use numpy to implement this equation next_h = tanh(Wx*x + Wh*h_prev + b)\n","        z = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n","        next_h = np.tanh(z)\n","        \n","        # Cache all needed values for backpropagation\n","        cache = (x, h_prev, Wx, Wh, b, z)\n","        \n","        #############################################################################################################\n","        \n","        return next_h, cache\n","\n","    \n","    \n","    def one_step_backward(self, dnext_h, cache):\n","        \"\"\"\n","        This function implements the backward pass of a single time step RNN\n","        \n","        Inputs:\n","        - dnext_h: the gradient of the loss with respect to the next hidden state of shape (N, H)\n","        - cache: a tupple that we cached before from the forward pass\n","        \n","        Returns:\n","        - dprev_h: Gradients of previous hidden state, of shape (N, H)\n","        - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n","        - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n","        - db: Gradients of bias vector, of shape (H,)\n","        \"\"\"\n","        dprev_h, dWx, dWh, db = None, None, None, None\n","        x, prev_h, Wx, Wh, b, tanh = cache\n","        \n","        ################# TODO: Impelement the backward pass by calculating derivatives and applying chain rule ###########\n","        # Backpropagate through tanh\n","        dtanh = dnext_h * (1 - np.tanh(tanh)**2)\n","\n","        # Gradient of the loss with respect to the previous hidden state\n","        dprev_h = np.dot(dtanh, Wh.T)\n","\n","        # Gradient of the loss with respect to the input-to-hidden weights\n","        dWx = np.dot(x.T, dtanh)\n","\n","        # Gradient of the loss with respect to the hidden-to-hidden weights\n","        dWh = np.dot(prev_h.T, dtanh)\n","\n","        # Gradient of the loss with respect to the bias vector\n","        db = np.sum(dtanh, axis=0)\n","\n","        ###################################################################################################################\n","\n","        return dprev_h, dWx, dWh, db\n","        \n","        \n","        ###################################################################################################################\n","        \n","        return dprev_h, dWx, dWh, db\n","    \n","    \n","    def full_forward_pass(self, x, h0, Wx, Wh, b):\n","        \"\"\"\n","        This function runs the RNN forward on an entire sequence of data. We assume an input\n","        sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n","        size of H, and we work over a minibatch containing N sequences. After running\n","        the RNN forward, we return the hidden states for all timesteps.\n","        \n","        Inputs:\n","        - x: Input data for the entire timeseries, of shape (N, T, D).\n","        - h0: Initial hidden state, of shape (N, H)\n","        - Wx: Weight matrix for input to hidden transformation, of shape (D, H)\n","        - Wh: Weight matrix for hidden to hidden transformation, of shape (H, H)\n","        - b: Biases of shape (H,)\n","        \n","        Returns a tuple of:\n","        - h: Hidden states for the entire timeseries, of shape (N, T, H).\n","        - cache: All values needed in the backward pass\n","        \"\"\"\n","        h, cache = None, None\n","        N, T, D = x.shape\n","        H = h0.shape[1]\n","        #################### TODO: Implement the full forward pass of the RNN ###########################\n","        # Hint 1: You will use the one_step_forward to compute each time step hidden state and cache\n","        # Hint 2: For loop over T and call one_step_forward with the appropriate parameters\n","        \n","        # create numpy array filled with zeros of shape (N, T, H) for h to be filled then returned\n","        h = np.zeros((N, T, H))\n","        \n","        # initialize the cache as an empty list to be filled with all caches returned from one_step_forward\n","        cache = []\n","        \n","        \n","        # loop over T and calculate the needed hidden states and caches\n","        for t in range(T):\n","            # Compute the next hidden state and cache\n","            h[:, t, :], cache_t = self.one_step_forward(x[:,t, :], h0, Wx, Wh, b)\n","            cache.append(cache_t)\n","            h0 = h[:, t, :]\n","        \n","        #################################################################################################\n","        \n","        return h, cache\n","    \n","    \n","    def full_backward_pass(self, dh, cache):\n","        \"\"\"\n","        This function computes the backward pass for the RNN over an entire sequence of data.\n","        Inputs:\n","        - dh: The loss gradient with respect to all hidden states, of shape (N, T, H). \n","\n","        Returns a tuple of:\n","        - dh0: Gradient of initial hidden state, of shape (N, H)\n","        - dWx: Gradient of input to hidden weights, of shape (D, H)\n","        - dWh: Gradient of hidden to hidden weights, of shape (H, H)\n","        - db: Gradient of biases, of shape (H,)\n","        \"\"\"\n","        dh0, dWx, dWh, db = None, None, None, None\n","        N, T, H = dh.shape\n","        \n","        #################### TODO: Implement the full backward pass of the RNN ###########################\n","        # Hint 1: You will use the one_step_backward to compute each time step gradients\n","        # Hint 2: For loop over T and call one_step_backward with the appropriate parameters\n","        # Hint 3: Don't forget to add dprev_h returned from one_step_backward to the appropriate index in dh while passing\n","        #         to one_step_backward\n","        D = cache[0][2].shape[0]\n","        dWx = np.zeros((D, H))\n","        dWh = np.zeros((H, H))\n","        db = np.zeros(H)\n","\n","        dh0 = dh[:, T-1, :]\n","        for t in range(T-1, -1, -1):\n","            dh0, current_dWx, current_dWh, current_db = self.one_step_backward(dh0, cache[t])\n","            if t != 0:\n","                dh0 += dh[:, t-1, :]\n","            dWx += current_dWx\n","            dWh += current_dWh\n","            db += current_db\n","        \n","        ##################################################################################################\n","        \n","        return dh0, dWx, dWh, db\n","    \n","    def hidden_to_scores_forward(self, h, Wy, by):\n","        \"\"\"\n","        This function implements the forward pass of transforming the hidden state to the scores vector.\n","        The input is a set of H-dimensional vectors arranged into a minibatch of N timeseries, each of length T.\n","        The goal is transform from the hidden state to the scores output vector with size V (number of vocabulary).\n","        \n","        Inputs:\n","        - h: Input data of shape (N, T, H)\n","        - Wy: Weights of shape (H, V)\n","        - by: Biases of shape (V,)\n","       \n","        Returns a tuple of:\n","        - y: Output data of shape (N, T, V)\n","        - cache: Values needed for the backward pass\n","        \"\"\"\n","        N, T, H = h.shape\n","        V = by.shape[0]\n","        \n","        y, cache = None, None\n","        \n","        ##################### TODO: calculate the scores matrix y and cache all needed values ########################\n","        # Hint 1: you need to caculate this formula y = Wy*h + by\n","        # Hint 2: you can reshape h from (N, T, H) to (N*T, H) before the matrix multiplication\n","        #         then reshape it back to (N, T, H) after the matrix multiplication and before adding the by\n","        \n","        y = np.dot(h.reshape(N*T, H), Wy) + by\n","        \n","        # Reshape y back to (N, T, V)\n","        y = y.reshape(N, T, V)\n","        \n","        # Cache values for backward pass\n","        cache = (h, Wy, by, y)\n","        \n","        ##############################################################################################################\n","        \n","        return y, cache\n","    \n","    \n","    def hidden_to_scores_backward(self, dy, cache):\n","        \"\"\"\n","        Backward pass for the hidden to scores layer.\n","        Input:\n","        - dy: The gradients of shape (N, T, V)\n","        - cache: Values from forward pass\n","        Returns a tuple of:\n","        - dh: Gradient of input, of shape (N, T, H)\n","        - dWy: Gradient of weights, of shape (H, V)\n","        - dby: Gradient of biases, of shape (V,)\n","        \"\"\"\n","        h, Wy, by, y = cache\n","        N, T, H = h.shape\n","        V = by.shape[0]\n","\n","        dh, dWy, dby = None, None, None\n","        \n","        ################################ TODO: calculate the gradients ##############################\n","        # Hint: use reshape to ensure correct dimensions for matrix multiplications and derivat\n","        dh = np.dot(dy.reshape(N * T, V), Wy.T)\n","        dh = dh.reshape(N, T, H)\n","\n","        dWy = np.dot(h.reshape(N * T, H).T, dy.reshape(N*T, V))\n","        dby = np.sum(dy, axis=(0, 1))\n","\n","        #############################################################################################\n","\n","        return dh, dWy, dby\n","    \n","    def softmax_loss(self, x, y, mask):\n","        \"\"\"\n","        This function calculates the softmax loss over minibatches of size N.\n","        \n","        Inputs:\n","        - x: Input scores, of shape (N, T, V)\n","        - y: Ground-truth indices, of shape (N, T) where each element is in the range\n","             0 <= y[i, t] < V\n","        - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n","          the scores at x[i, t] should contribute to the loss.\n","        Returns a tuple of:\n","        - loss: Scalar giving loss\n","        - dx: Gradient of loss with respect to scores x, of shape (N, T, V).\n","        \"\"\"\n","        N, T, V = x.shape\n","        x_flat = x.reshape(N * T, V)\n","        y_flat = y.reshape(N * T)\n","        mask_flat = mask.reshape(N * T)\n","        probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))\n","        probs /= np.sum(probs, axis=1, keepdims=True)\n","        loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n","        dx_flat = probs.copy()\n","        dx_flat[np.arange(N * T), y_flat] -= 1\n","        dx_flat /= N\n","        dx_flat *= mask_flat[:, None]\n","        dx = dx_flat.reshape(N, T, V)\n","        return loss, dx\n","    \n","    \n","    def word_embeddings_forward(self, x_sentences, W_embedd):\n","        \"\"\"\n","        This function takes input with tokens as indexes to produce their word embeddings\n","        \n","        Inputs:\n","        - x_sentences: input sentences of shape (N, T)\n","        - W_embedd: embeddings matrix of size (V, V)\n","        \n","        Returns:\n","        - x: matrix with word embeddings of shape (N, T, V)\n","        \"\"\"\n","        return W_embedd[x_sentences]\n","    \n","    \n","    def loss(self, sentences):\n","        \"\"\"\n","        This function computes the loss for training and the gradients of all RNN weights.\n","        \n","        Inputs:\n","        - sentences: Ground-truth sentences; an integer array of shape (N, T) where\n","          each element is in the range 0 <= y[i, t] < V\n","        Returns a tuple of:\n","        - loss: Scalar loss\n","        - grads: Dictionary of gradients for all RNN weights\n","        \"\"\"\n","        # Since we are training a language model, for each token input to the RNN we should predict the\n","        # next token. So the inputs will the all tokens from begin to end -1 and the outputs will be all tokens\n","        # except the first token\n","        sentences_in = sentences[:, :-1]\n","        sentences_out = sentences[:, 1:]\n","\n","        # You'll need this\n","        mask = (sentences_out != self.null_word_index)\n","\n","        # Word embedding matrix\n","        W_embed = self.embeddings\n","\n","        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n","        Wx, Wh, b = self.Wx, self.Wh, self.b\n","\n","        # Weight and bias for the hidden-to-vocab transformation.\n","        Wy, by = self.Wy, self.by\n","        \n","        # sizes\n","        N, T = sentences.shape\n","        V, H = Wx.shape\n","\n","        loss, grads = 0.0, {'Wx':None, 'Wh':None, 'b':None, 'Wy':None, 'by':None}\n","        \n","        ###################### TODO: Implement the forward and backward passes for the RNN LM #########################\n","        # In the forward pass you will need to do the following:                   \n","        \n","        # (1) Initilize the initial hidden state h0 to a zeros matrix of size (N, H)\n","        h0 = np.zeros((N, H))\n","        \n","        # (2) Use a word embedding layer to transform the words in captions_in  \n","        #     from indices to vectors, giving an array of shape (N, T, W).\n","        x = self.word_embeddings_forward(sentences_in, W_embed)\n","\n","        \n","        # (3) Call rnn_forward with the appropriate parameters to produce\n","        #     array of hidden states with shape (N, T, H)\n","        h, cache_rnn = self.full_forward_pass(x, h0, Wx, Wh, b)\n","        \n","             \n","        # (4) Call hidden_to_scores_forward to get an array of scores of shape (N, T, V)\n","        scores, cache_scores = self.hidden_to_scores_forward(h, Wy, by)\n","        \n","        # (5) Call softmax_loss to compute loss using captions_out, ignoring\n","        #     the points where the output word is <NULL> using the mask above.\n","        loss, dscores = self.softmax_loss(scores, sentences_out, mask)\n","        \n","        \n","        # In the backward pass you will need to compute the gradient of the loss\n","        # with respect to all model parameters. Use the loss and grads variables\n","        # defined above to store loss and gradients; grads['k'] should give the \n","        # gradients for self..\n","\n","        # (1) Call hidden_to_scores_backward\n","        dh, dWy, dby = self.hidden_to_scores_backward(dscores, cache_scores)\n","\n","\n","        # (2) Call full_backward_pass\n","        dh0, dWx, dWh, db = self.full_backward_pass(dh, cache_rnn)\n","\n","\n","        # (3) save the gradients in the grads dictionary\n","        grads = {'Wx':dWx, 'Wh':dWh, 'b':db, 'Wy':dWy, 'by':dby}\n","        ###############################################################################################################\n","\n","        return loss, grads\n","    \n","    def preprocessDataset(self, train_set):\n","        train_set = [[self.word2id[word] for word in s] for s in train_set]\n","        max_length = np.max([len(s) for s in train_set])\n","        train_set = [s + [self.null_word_index] * (max_length - len(s)) for s in train_set]\n","        return np.array(train_set, dtype=int)\n","    \n","    def train(self, sentences, batch_size=64, num_iterations=100, lr=0.0001):     \n","        for iteration in range(num_iterations):\n","            index = 0\n","            while index < len(sentences):\n","                loss, grads = self.loss(sentences[index: index + batch_size])\n","                self.Wx -= lr * grads['Wx']\n","                self.Wh -= lr * grads['Wh']\n","                self.b -= lr * grads['b']\n","                self.Wy -= lr * grads['Wy']\n","                self.by -= lr * grads['by']\n","                index += batch_size\n","            print(f\"Iteration: {iteration} | loss = {loss}\")\n","            \n","    def predict(self, tokenized_sentence):\n","        idx = tokenized_sentence.index(\"mask\")\n","        prev_h = np.zeros((1, self.Wh.shape[0]), dtype=np.float64)\n","        x = self.word_embeddings_forward([self.word2id[token] for token in tokenized_sentence[:idx]], self.embeddings).reshape(1, idx, self.vocab_size)\n","        for i in range(idx):\n","            prev_h, cache_h = self.one_step_forward(x[:,i,:], prev_h, self.Wx, self.Wh, self.b)\n","        out, cache_voc = self.hidden_to_scores_forward(prev_h.reshape(1, 1, -1), self.Wy, self.by)\n","        return self.id2word[np.argmax(out)]\n","    \n","    \n","    def sample(self, token, prev_h):\n","        \"\"\"\n","        This function takes a token and previous hidden state and samples the next token based on the softmax\n","        Probability distribution\n","        \n","        Inputs:\n","        - token: string containing the current token\n","        - prev_h: the previous hidden state of shape (1, H)\n","        \n","        Returns:\n","        - next_token: string containing the sampled next token\n","        - curr_h: the produced hidden state from the RNN\n","        \"\"\"\n","        x = self.word_embeddings_forward([self.word2id[token]], self.embeddings).reshape(1, 1, -1)  # Ensure x has the right shape\n","        Wx, Wh, b = self.Wx, self.Wh, self.b\n","        Wy, by = self.Wy, self.by\n","        \n","        next_token, curr_h = None, None\n","        ############################ TODO: implement the sampling ############################################\n","        # Call one_step_forward with the appropriate parameters\n","        next_h, _ = self.one_step_forward(x, prev_h, Wx, Wh, b)\n","        \n","        # Call hidden_to_scores_forward to produce the scores\n","        scores, _ = self.hidden_to_scores_forward(next_h, Wy, by)\n","\n","        # Compute the softmax manually\n","        probs = np.exp(scores - np.max(scores)) / np.sum(np.exp(scores - np.max(scores)), axis=-1, keepdims=True)\n","\n","        # Sample the token using np.random.choice\n","        next_token_id = np.random.choice(len(probs.ravel()), p=probs.ravel())\n","        next_token = self.id2word[next_token_id]\n","\n","        ######################################################################################################\n","        return next_token, next_h\n","\n","    def generateOrder(self):\n","        prev_h = np.zeros((1, self.Wh.shape[0]), dtype=np.float64)\n","        tokens = []\n","        t = '<s>'\n","        while t != '</s>':\n","            t, prev_h = self.sample(t, prev_h)\n","            tokens.append(t)\n","        return ' '.join(tokens[:-1])"]},{"cell_type":"markdown","id":"33f0e26d","metadata":{"id":"33f0e26d"},"source":["# Let's Test Your Code\n","The next code cell implements functions for testing"]},{"cell_type":"code","execution_count":40,"id":"03b1f075","metadata":{"id":"03b1f075"},"outputs":[],"source":["def rel_error(x, y):\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","    \"\"\"\n","    a naive implementation of numerical gradient of f at x\n","    - f should be a function that takes a single argument\n","    - x is the point (numpy array) to evaluate the gradient at\n","    \"\"\"\n","\n","    fx = f(x) # evaluate function value at original point\n","    grad = np.zeros_like(x)\n","    # iterate over all indexes in x\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","\n","        # evaluate function at x+h\n","        ix = it.multi_index\n","        oldval = x[ix]\n","        x[ix] = oldval + h # increment by h\n","        fxph = f(x) # evalute f(x + h)\n","        x[ix] = oldval - h\n","        fxmh = f(x) # evaluate f(x - h)\n","        x[ix] = oldval # restore\n","\n","        # compute the partial derivative with centered formula\n","        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","        if verbose:\n","            print(ix, grad[ix])\n","        it.iternext() # step to next dimension\n","\n","    return grad\n","\n","\n","def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","    \"\"\"\n","    Evaluate a numeric gradient for a function that accepts a numpy\n","    array and returns a numpy array.\n","    \"\"\"\n","    grad = np.zeros_like(x)\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        ix = it.multi_index\n","\n","        oldval = x[ix]\n","        x[ix] = oldval + h\n","        pos = f(x).copy()\n","        x[ix] = oldval - h\n","        neg = f(x).copy()\n","        x[ix] = oldval\n","\n","        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","        it.iternext()\n","    return grad"]},{"cell_type":"markdown","id":"25645083","metadata":{"id":"25645083"},"source":["# Test One Step Forward"]},{"cell_type":"code","execution_count":41,"id":"ea6e4d37","metadata":{"id":"ea6e4d37"},"outputs":[{"name":"stdout","output_type":"stream","text":["next_h error:  6.292421426471037e-09\n"]}],"source":["N, D, H = 3, 10, 4\n","rnnLM = RNNLM(vocab)\n","x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n","prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n","b = np.linspace(-0.2, 0.4, num=H)\n","\n","next_h, _ = rnnLM.one_step_forward(x, prev_h, Wx, Wh, b)\n","expected_next_h = np.asarray([\n","  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n","  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n","  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n","\n","assert rel_error(expected_next_h, next_h) < 1e-8, \"Error in one_step_forward\"\n","print('next_h error: ', rel_error(expected_next_h, next_h))"]},{"cell_type":"markdown","id":"ea924f7f","metadata":{"id":"ea924f7f"},"source":["# Test One Step Backwad"]},{"cell_type":"code","execution_count":42,"id":"a00b1fb6","metadata":{"id":"a00b1fb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["dprev_h error:  2.5658753744307165e-10\n","dWx error:  8.845193511067368e-10\n","dWh error:  1.6263476408377522e-10\n","db error:  4.584901020876022e-11\n"]}],"source":["rnnLM = RNNLM(vocab)\n","np.random.seed(231)\n","N, D, H = 4, 5, 6\n","x = np.random.randn(N, D)\n","h = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnnLM.one_step_forward(x, h, Wx, Wh, b)\n","\n","dnext_h = np.random.randn(*out.shape)\n","\n","fx = lambda x: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fh = lambda prev_h: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fb = lambda b: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n","dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n","db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n","\n","dprev_h, dWx, dWh, db = rnnLM.one_step_backward(dnext_h, cache)\n","\n","assert rel_error(dprev_h_num, dprev_h) < 1e-9, \"Error in one_step_backward\"\n","assert rel_error(dWx_num, dWx) < 1e-9, \"Error in one_step_backward\"\n","assert rel_error(dWh_num, dWh) < 1e-9, \"Error in one_step_backward\"\n","assert rel_error(db_num, db) < 1e-9, \"Error in one_step_backward\"\n","print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","id":"00bc0f72","metadata":{"id":"00bc0f72"},"source":["# Test full forward pass"]},{"cell_type":"code","execution_count":43,"id":"e8ecd42f","metadata":{"id":"e8ecd42f"},"outputs":[{"name":"stdout","output_type":"stream","text":["h error:  7.728466180186066e-08\n"]}],"source":["N, T, D, H = 2, 3, 4, 5\n","rnnLM = RNNLM(vocab)\n","\n","x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n","h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n","b = np.linspace(-0.7, 0.1, num=H)\n","\n","h, _ = rnnLM.full_forward_pass(x, h0, Wx, Wh, b)\n","expected_h = np.asarray([\n","  [\n","    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n","    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n","    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n","  ],\n","  [\n","    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n","    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n","    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n","assert rel_error(expected_h, h) < 1e-7, \"Error in full forward pass\"\n","print('h error: ', rel_error(expected_h, h))"]},{"cell_type":"markdown","id":"25d21145","metadata":{"id":"25d21145"},"source":["# Test full backward pass"]},{"cell_type":"code","execution_count":44,"id":"b37b6669","metadata":{"id":"b37b6669"},"outputs":[{"name":"stdout","output_type":"stream","text":["dh0 error:  6.453285937992153e-11\n","dWx error:  4.642588113556617e-10\n","dWh error:  8.14374529514666e-10\n","db error:  3.9550822355654096e-11\n"]}],"source":["np.random.seed(231)\n","rnnLM = RNNLM(vocab)\n","N, D, T, H = 2, 3, 10, 5\n","\n","x = np.random.randn(N, T, D)\n","h0 = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnnLM.full_forward_pass(x, h0, Wx, Wh, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","dh0, dWx, dWh, db = rnnLM.full_backward_pass(dout, cache)\n","\n","fx = lambda x: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fh0 = lambda h0: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fb = lambda b: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","\n","dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","assert rel_error(dh0_num, dh0) < 1e-9, \"Error in full_backward_pass\"\n","assert rel_error(dWx_num, dWx) < 1e-9, \"Error in full_backward_pass\"\n","assert rel_error(dWh_num, dWh) < 1e-9, \"Error in full_backward_pass\"\n","assert rel_error(db_num, db) < 1e-9, \"Error in full_backward_pass\"\n","\n","print('dh0 error: ', rel_error(dh0_num, dh0))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","id":"4420a0c7","metadata":{"id":"4420a0c7"},"source":["# Check hidden to scores forward and backward"]},{"cell_type":"code","execution_count":45,"id":"97aa4935","metadata":{"id":"97aa4935"},"outputs":[{"name":"stdout","output_type":"stream","text":["dx error:  6.848770688344809e-10\n","dw error:  8.503919884552969e-11\n","db error:  7.534133932432161e-12\n"]}],"source":["# Gradient check for temporal affine layer\n","N, T, D, M = 2, 3, 4, 5\n","rnnLM = RNNLM(vocab)\n","x = np.random.randn(N, T, D)\n","w = np.random.randn(D, M)\n","b = np.random.randn(M)\n","\n","out, cache = rnnLM.hidden_to_scores_forward(x, w, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","fx = lambda x: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n","fw = lambda w: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n","fb = lambda b: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dw_num = eval_numerical_gradient_array(fw, w, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","dx, dw, db = rnnLM.hidden_to_scores_backward(dout, cache)\n","\n","assert rel_error(dx_num, dx), \"Error in hidden to scores\"\n","assert rel_error(dw_num, dw), \"Error in hidden to scores\"\n","assert rel_error(db_num, db), \"Error in hidden to scores\"\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dw error: ', rel_error(dw_num, dw))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","id":"d199e23a","metadata":{"id":"d199e23a"},"source":["# Check the loss function"]},{"cell_type":"code","execution_count":46,"id":"0ba3e645","metadata":{"id":"0ba3e645"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wx relative error: 1.759454e-08\n","Wh relative error: 8.887021e-07\n","b relative error: 3.626057e-07\n","Wy relative error: 1.386084e-05\n","by relative error: 3.653445e-10\n"]}],"source":["N, H = 5, 20\n","test_vocab = {'', 'cat', 'dog'}\n","V = len(test_vocab)\n","T = 10\n","\n","model = RNNLM(test_vocab, H)\n","\n","sentences_test = (np.arange(N * T) % V).reshape(N, T)\n","\n","loss, grads = model.loss(sentences_test)\n","\n","\n","f = lambda _: model.loss(sentences_test)[0]\n","param_grad_num = eval_numerical_gradient(f, model.Wx, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['Wx'])\n","print('%s relative error: %e' % ('Wx', e))\n","assert e < 1e-6, \"Error in dWx\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.Wh, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['Wh'])\n","print('%s relative error: %e' % ('Wh', e))\n","assert e < 1e-6, \"Error in dWh\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.b, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['b'])\n","print('%s relative error: %e' % ('b', e))\n","assert e < 1e-6, \"Error in db\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.Wy, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['Wy'])\n","print('%s relative error: %e' % ('Wy', e))\n","assert e < 1e-4, \"Error in dWy\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.by, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['by'])\n","print('%s relative error: %e' % ('by', e))\n","assert e < 1e-6, \"Error in dby\""]},{"cell_type":"markdown","id":"60c23262","metadata":{"id":"60c23262"},"source":["# The following cells will produce the output files"]},{"cell_type":"code","execution_count":47,"id":"a08d7556","metadata":{"id":"a08d7556"},"outputs":[],"source":["with open(f'{team_ID}.txt', 'w') as f:\n","    f.write(team_ID + '\\n')\n","    f.write(Student1_Name + '\\n')\n","    f.write(Student2_Name + '\\n')"]},{"cell_type":"code","execution_count":48,"id":"4d10eec2","metadata":{"id":"4d10eec2"},"outputs":[],"source":["bigramLM = BigramLM(vocab)\n","bigramLM.train(preprocessed_train_sentences)\n","\n","predictions_bigram = []\n","for sentence in preprocessed_test_sentences:\n","    predictions_bigram.append(bigramLM.predict(sentence))\n","\n","pd.DataFrame({'predictions': predictions_bigram}).to_csv(f'{team_ID}_bigram_predictions.csv', index=False)"]},{"cell_type":"code","execution_count":49,"id":"6c2ba40e","metadata":{"id":"6c2ba40e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 0 | loss = 59.65986599117875\n","Iteration: 1 | loss = 52.02646153233303\n","Iteration: 2 | loss = 44.974532995511495\n","Iteration: 3 | loss = 39.2633468099842\n","Iteration: 4 | loss = 35.886485905291416\n","Iteration: 5 | loss = 33.445500676319824\n","Iteration: 6 | loss = 31.464774382975815\n","Iteration: 7 | loss = 29.848040150326945\n","Iteration: 8 | loss = 28.527650568305393\n","Iteration: 9 | loss = 27.447383194992938\n","Iteration: 10 | loss = 26.548291402101544\n","Iteration: 11 | loss = 25.784213715787345\n","Iteration: 12 | loss = 25.130208905508535\n","Iteration: 13 | loss = 24.56266771533047\n","Iteration: 14 | loss = 24.06041723843898\n","Iteration: 15 | loss = 23.609635617922063\n","Iteration: 16 | loss = 23.199490068354223\n","Iteration: 17 | loss = 22.82172291756153\n","Iteration: 18 | loss = 22.47062912437665\n","Iteration: 19 | loss = 22.142858723479126\n"]}],"source":["rnnLM = RNNLM(vocab)\n","train_set_RNN = rnnLM.preprocessDataset(preprocessed_train_sentences)\n","rnnLM.train(train_set_RNN, lr=0.01, num_iterations=20)\n","\n","predictions_rnn = []\n","for sentence in preprocessed_test_sentences:\n","    predictions_rnn.append(rnnLM.predict(sentence))\n","\n","pd.DataFrame({'predictions': predictions_rnn}).to_csv(f'{team_ID}_rnn_predictions.csv', index=False)"]},{"cell_type":"code","execution_count":50,"id":"5c888167","metadata":{"id":"5c888167"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bigram prediction Accuracy =  0.397\n","RNN prediction Accuracy =  0.46\n"]}],"source":["gold_labels = pd.read_csv('labels.csv')\n","bigram_predictions = pd.read_csv(f'{team_ID}_bigram_predictions.csv')\n","rnn_predictions = pd.read_csv(f'{team_ID}_rnn_predictions.csv')\n","\n","bigram_acc = np.where(bigram_predictions['predictions'] == gold_labels['labels'], True, False)\n","bigram_acc = np.sum(bigram_acc) / len(bigram_acc)\n","\n","rnn_acc = np.where(rnn_predictions['predictions'] == gold_labels['labels'], True, False)\n","rnn_acc = np.sum(rnn_acc) / len(rnn_acc)\n","\n","print(\"Bigram prediction Accuracy = \", bigram_acc)\n","print(\"RNN prediction Accuracy = \", rnn_acc)"]},{"cell_type":"markdown","id":"48c69d50","metadata":{"id":"48c69d50"},"source":["# For your fun"]},{"cell_type":"code","execution_count":51,"id":"74356f27","metadata":{"id":"74356f27"},"outputs":[{"name":"stdout","output_type":"stream","text":["pizza with cherry tomato thirteen 2 iced teas \n","\n","i 'd like two-liter wood pepsi and three party pineaple american cheese and three liter pellegrinos \n","\n","i 'd like twelve pizza with peperroni have bottle peppperonis cans of dried tomatoes balzamic glaze and five fantas and five personal sized pizzas with not tunas can powder coke zeros and black dr peppers without spiced 5 doctor cheeseburger pepperonis peppers and five lunch sized pies with banana toppings tuna six ranch eleven pizza with peppperoni and pea can i 'd like a bottle mushroom parmesan and three party size pizzas no fried dough pie garlic spicy i want four 200 milliliter lettuce applewood small pepsis cokes jalapeno pepper \n","\n","i fl ounce iced teas and dried pepper hold the pepperonis diet beans and a pizza with banana peppers \n","\n","i want brocoli of american cheese and two 200 any applewood apple american cheese and five pies with extra pepperonis ice teas zeros and a ginger ale and with thin meatballs diet eleven free 4 italian shrimps tomatoes pies with american cheese and three extra zeros white onions ales \n","\n","a bottle of ham \n","\n","i 'd like a fanta bit cherry pineapples ounce perriers medium fantas shrimps zeroes and pecorino cheese and a party - apple fat carrot with dr 1 pies with extra american zero meatlovers meatball neapolitan veggie thick much chickens brocoli anchovies mushroom stuffed three party - sized pizzas with thin crust sixteen any artichoke banana chicken up and rosemary hold zeroes artichokes hawaiian lot style any lettuce pickle perrier applewood bacon \n","\n","one one party size pizzas with balsamic glaze and five three pizzas with american cheese \n","\n","a lot wood bbq chicken bacon fanta vegan pizza with american cheese \n","\n","a pizza with american cheese and a ginger sausage spinach pan glaze and a bottle of caramelized onion and a sprite \n","\n"]}],"source":["bigram_generation = []\n","np.random.seed(7)\n","for _ in range(10):\n","    s = bigramLM.generateOrder()\n","    bigram_generation.append(s)\n","    print(s)\n","    print()\n","\n","pd.DataFrame({'generations': bigram_generation}).to_csv(f'{team_ID}_bigram_generations.csv', index=False)"]},{"cell_type":"code","execution_count":52,"id":"e8c3d102","metadata":{"id":"e8c3d102"},"outputs":[{"name":"stdout","output_type":"stream","text":["pizza with no cheese and with just yellow\n","\n","i 'd like one pizza with jalapeno pie and roasted pepper\n","\n","a sprite and a 16 liter zeroes coffees diet\n","\n","i 'd like 500 stuffed crust crusts basil\n","\n","i 'd like lunch pizza with onions and 13 ginger pineapple\n","\n","i 'd like balsamic pizza with feta and a sprite of up\n","\n","five medium cans sauce\n","\n","four pizzas with balsamic glaze and two party sized pies with only a olives bit chicken\n","\n","four seven ups and a zero mountain lemon pepsi small pellegrino\n","\n","i 'd like a pizza with pecorino sauce and sausage olives with pineapple crust\n","\n"]}],"source":["rnn_generation = []\n","np.random.seed(7)\n","for _ in range(10):\n","    s = rnnLM.generateOrder()\n","    rnn_generation.append(s)\n","    print(s)\n","    print()\n","\n","pd.DataFrame({'generations': rnn_generation}).to_csv(f'{team_ID}_rnn_generations.csv', index=False)"]},{"cell_type":"markdown","id":"9e370b33","metadata":{"id":"9e370b33"},"source":["# Thank you for your efforts :D"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
