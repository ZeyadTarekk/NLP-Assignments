{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "PXjGQSypydnl"
      },
      "outputs": [],
      "source": [
        "# Type your full names\n",
        "Student_1 = \"Zeyad Tarek Khairy\"\n",
        "Student_2 = \"Asmaa Adel Abdelhamed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVqrsVWh0kiC"
      },
      "source": [
        "# Named Entity Recognition Assignment\n",
        "NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. In this assignment, you will train a named entity recognition system and test it on a test data. \\\n",
        "Let's get started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WR6a6DkN0d-3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from utils import get_params, get_vocab\n",
        "import random as rnd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_44BK5K82YwF"
      },
      "source": [
        "# Importing and discovering the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ulSik2Sv1p1G"
      },
      "outputs": [],
      "source": [
        "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
        "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
        "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
        "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PkKTK22xc6"
      },
      "source": [
        "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token.\n",
        "\n",
        "When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IB_1MhtP1rLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6265aa3-8ab8-44bd-b54a-625b6dcf8b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab[\"the\"]: 9\n",
            "padded token: 35180\n"
          ]
        }
      ],
      "source": [
        "# vocab translates from a word to a unique number\n",
        "print('vocab[\"the\"]:', vocab[\"the\"])\n",
        "# Pad token\n",
        "print('padded token:', vocab['<PAD>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PagjN4rl22Fr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfb32ac-80d1-400a-d93e-df07daaeebf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
          ]
        }
      ],
      "source": [
        "# The possible tags\n",
        "print(tag_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6e9sGin3FQ-"
      },
      "source": [
        "So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If you had the sentence\n",
        "\n",
        "**\"Sharon flew to Miami on Friday\"**\n",
        "\n",
        "the outputs would look like:\n",
        "\n",
        "```\n",
        "Sharon B-per\n",
        "flew   O\n",
        "to     O\n",
        "Miami  B-geo\n",
        "on     O\n",
        "Friday B-tim\n",
        "```\n",
        "\n",
        "your tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence:\n",
        "\n",
        "**\"Sharon Floyd flew to Miami on Friday\"**\n",
        "\n",
        "```\n",
        "Sharon B-per\n",
        "Floyd  I-per\n",
        "flew   O\n",
        "to     O\n",
        "Miami  B-geo\n",
        "on     O\n",
        "Friday B-tim\n",
        "```\n",
        "\n",
        "then your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oWLR2Oxp28K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcf0527-dd95-4511-8c67-e47272e29251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of outputs is tag_map 17\n",
            "Num of vocabulary words: 35181\n",
            "The vocab size is 35181\n",
            "The training size is 33570\n",
            "The validation size is 7194\n",
            "An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
            "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# Exploring information about the data\n",
        "print('The number of outputs is tag_map', len(tag_map))\n",
        "# The number of vocabulary tokens (including <PAD>)\n",
        "g_vocab_size = len(vocab)\n",
        "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
        "print('The vocab size is', len(vocab))\n",
        "print('The training size is', t_size)\n",
        "print('The validation size is', v_size)\n",
        "print('An example of the first sentence is', t_sentences[0])\n",
        "print('An example of its corresponding label is', t_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3e4nxjFT3O"
      },
      "source": [
        "# NERDataset\n",
        "The class that impelements the dataset for NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "29iM0u4-4YOV"
      },
      "outputs": [],
      "source": [
        "class NERDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, x, y, pad):\n",
        "    \"\"\"\n",
        "    This is the constructor of the NERDataset\n",
        "    Inputs:\n",
        "    - x: a list of lists where each list contains the ids of the tokens\n",
        "    - y: a list of lists where each list contains the label of each token in the sentence\n",
        "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
        "    \"\"\"\n",
        "    ##################### TODO: create two tensors one for x and the other for labels ###############################\n",
        "    self.x = nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(i) for i in x], batch_first=True, padding_value=pad)\n",
        "    self.y = nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(j) for j in y], batch_first=True, padding_value=0)\n",
        "\n",
        "    #################################################################################################################\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    This function should return the length of the dataset (the number of sentences)\n",
        "    \"\"\"\n",
        "    ###################### TODO: return the length of the dataset #############################\n",
        "    return len(self.x)\n",
        "    ###########################################################################################\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    This function returns a subset of the whole dataset\n",
        "    \"\"\"\n",
        "    ###################### TODO: return a tuple of x and y ###################################\n",
        "    return self.x[idx], self.y[idx]\n",
        "    ##########################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "sz-saCtRs7Pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498ea0c6-7587-4841-dbe1-60fb26c7781c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 30]) torch.Size([5, 30]) torch.Size([3, 30]) torch.Size([3, 30])\n",
            "tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
            "           10,    11,    12,    13,    14,     9,    15,     1,    16,    17,\n",
            "           18,    19,    20,    21, 35180, 35180, 35180, 35180, 35180, 35180]) \n",
            " tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "mini_sentences = t_sentences[0: 8]\n",
        "mini_labels = t_labels[0: 8]\n",
        "mini_dataset = NERDataset(mini_sentences, mini_labels, vocab['<PAD>'])\n",
        "dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n",
        "dg = iter(dummy_dataloader)\n",
        "X1, Y1 = next(dg)\n",
        "X2, Y2 = next(dg)\n",
        "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
        "print(X1[0][:], \"\\n\", Y1[0][:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfw9pEd1wdMr"
      },
      "source": [
        "#### Expected output\n",
        "torch.Size([5, 30]) torch.Size([5, 30]) torch.Size([3, 30]) torch.Size([3, 30])\\\n",
        "tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
        "           10,    11,    12,    13,    14,     9,    15,     1,    16,    17,\n",
        "           18,    19,    20,    21, 35180, 35180, 35180, 35180, 35180, 35180]) \\\n",
        "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQB6O7I7FbUh"
      },
      "source": [
        "# NER\n",
        "The class that implementss the pytorch model for NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xHeJcz1JuhYa"
      },
      "outputs": [],
      "source": [
        "class NER(nn.Module):\n",
        "  def __init__(self, vocab_size=35181, embedding_dim=50, hidden_size=50, n_classes=len(tag_map)):\n",
        "    \"\"\"\n",
        "    The constructor of our NER model\n",
        "    Inputs:\n",
        "    - vacab_size: the number of unique words\n",
        "    - embedding_dim: the embedding dimension\n",
        "    - n_classes: the number of final classes (tags)\n",
        "    \"\"\"\n",
        "    super(NER, self).__init__()\n",
        "    # (1) Create the embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "    # (3) Create a linear layer with number of neorons = n_classes\n",
        "    self.linear = nn.Linear(hidden_size, n_classes)\n",
        "    #####################################################################################################\n",
        "\n",
        "  def forward(self, sentences):\n",
        "    \"\"\"\n",
        "    This function does the forward pass of our model\n",
        "    Inputs:\n",
        "    - sentences: tensor of shape (batch_size, max_length)\n",
        "\n",
        "    Returns:\n",
        "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
        "    \"\"\"\n",
        "    final_output = None\n",
        "    ######################### TODO: implement the forward pass ####################################\n",
        "    embeddingsOutput = self.embedding(sentences)\n",
        "    lstmOutput, _ = self.lstm(embeddingsOutput)\n",
        "    final_output = self.linear(lstmOutput)\n",
        "    ###############################################################################################\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "lJJJF-qQA_wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fcba0a-7c09-4a22-bb2a-8d832f9cf24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER(\n",
            "  (embedding): Embedding(35181, 50)\n",
            "  (lstm): LSTM(50, 50, batch_first=True)\n",
            "  (linear): Linear(in_features=50, out_features=17, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NER()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCI_6UCRBk6N"
      },
      "source": [
        "#### Expected output\n",
        "NER( \\\n",
        "  (embedding): Embedding(35181, 50) \\\n",
        "  (lstm): LSTM(50, 50, batch_first=True) \\\n",
        "  (linear): Linear(in_features=50, out_features=17, bias=True) \\\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLHx_oHpFlSX"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-yvaq8i2CCLD"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataset, batch_size=512, epochs=5, learning_rate=0.01):\n",
        "  \"\"\"\n",
        "  This function implements the training logic\n",
        "  Inputs:\n",
        "  - model: the model ot be trained\n",
        "  - train_dataset: the training set of type NERDataset\n",
        "  - batch_size: integer represents the number of examples per step\n",
        "  - epochs: integer represents the total number of epochs (full training pass)\n",
        "  - learning_rate: the learning rate to be used by the optimizer\n",
        "  \"\"\"\n",
        "\n",
        "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # (2) make the criterion cross entropy loss\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # (3) create the optimizer (Adam)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # GPU configuration\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "  for epoch_num in range(epochs):\n",
        "    total_acc_train = 0\n",
        "    total_loss_train = 0\n",
        "\n",
        "    for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "      # (4) move the train input to the device\n",
        "      train_input = train_input.to(device)\n",
        "\n",
        "      # (5) move the train label to the device\n",
        "      train_label = train_label.to(device)\n",
        "\n",
        "      # (6) do the forward pass\n",
        "      output = model(train_input)\n",
        "\n",
        "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
        "      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n",
        "\n",
        "      # (8) append the batch loss to the total_loss_train\n",
        "      total_loss_train += batch_loss.item()\n",
        "\n",
        "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
        "      acc = (output.argmax(dim=-1) == train_label).sum().item()\n",
        "      total_acc_train += acc\n",
        "\n",
        "      # (10) zero your gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # (11) do the backward pass\n",
        "      batch_loss.backward()\n",
        "\n",
        "      # (12) update the weights with your optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "    # epoch loss\n",
        "    epoch_loss = total_loss_train / len(train_dataset)\n",
        "\n",
        "    # (13) calculate the accuracy\n",
        "    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n",
        "\n",
        "    print(\n",
        "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
        "        | Train Accuracy: {epoch_acc}\\n')\n",
        "\n",
        "  ##############################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "3BI7_ANkLf7G"
      },
      "outputs": [],
      "source": [
        "train_dataset = NERDataset(t_sentences, t_labels, vocab['<PAD>'])\n",
        "val_dataset = NERDataset(v_sentences, v_labels, vocab['<PAD>'])\n",
        "test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "LMXjDv51LU6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eccb6e7-fbdf-4da6-b1e4-c07549526d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:00<00:00, 89.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss: 0.0005922412555825685         | Train Accuracy: 0.9458682775371784\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:00<00:00, 95.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss: 0.0001630664746215168         | Train Accuracy: 0.975783380307509\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:00<00:00, 71.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss: 0.00010770456873752689         | Train Accuracy: 0.9850856992277904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:00<00:00, 93.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss: 7.347101509340256e-05         | Train Accuracy: 0.9900168419605417\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:00<00:00, 103.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss: 5.839839505543835e-05         | Train Accuracy: 0.99189437684746\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train(model, train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko-pv4AOYDMT"
      },
      "source": [
        "#### Expected train accuracy after 5 epochs to be above 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJNO6mUXPRI"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Gz5mxUAJM1xS"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataset, batch_size=256):\n",
        "  \"\"\"\n",
        "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
        "  Inputs:\n",
        "  - model: a NER model\n",
        "  - test_dataset: dataset of type NERDataset\n",
        "  \"\"\"\n",
        "  ########################### TODO: Replace the Nones in the following code ##########################\n",
        "\n",
        "  # (1) create the test data loader\n",
        "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # GPU Configuration\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "  total_acc_test = 0\n",
        "\n",
        "  # (2) disable gradients\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for test_input, test_label in tqdm(test_dataloader):\n",
        "      # (3) move the test input to the device\n",
        "      test_label = test_label.to(device)\n",
        "\n",
        "      # (4) move the test label to the device\n",
        "      test_input = test_input.to(device)\n",
        "\n",
        "      # (5) do the forward pass\n",
        "      output = model(test_input)\n",
        "\n",
        "      # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
        "      acc = (torch.argmax(output, dim=-1) == test_label).sum().item()\n",
        "      total_acc_test += acc\n",
        "\n",
        "    # (6) calculate the over all accuracy\n",
        "    total_acc_test /= (len(test_dataset) * test_dataset[0][0].shape[0])\n",
        "  ##################################################################################################\n",
        "\n",
        "\n",
        "  print(f'\\nTest Accuracy: {total_acc_test}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6FD8JNcHWmMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00bbd39-3424-4f0a-85d3-3c104c80e5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:00<00:00, 364.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.9850053616108662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYC6H_6PYDMU"
      },
      "source": [
        "#### Expected test accuracy to be above 0.98"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhbNZ1HVaLO_"
      },
      "source": [
        "# Thank you"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}