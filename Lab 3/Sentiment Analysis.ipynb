{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80701dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your names In this cell\n",
    "Student_1 = \"Zeyad Tarek Khairy\"\n",
    "Student_2 = \"Asmaa Adel Abdelhamed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d31e3f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "In this requirement, you will implement a sentiment analyser using twitter data. We will do it using two classifiers: Logistic Regression and Naive Bayes. Your goal will be to learn how to extract features from tweets and use sklearn to train and test your classifiers.\n",
    "Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d64d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a24ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\adela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the dataset from nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# stop words are common words that we don't want to include in our features\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a43cd8",
   "metadata": {},
   "source": [
    "# Prepare the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1be4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "print(len(train_x))\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181c7574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "\n",
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cadd2b4",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "You are given a utils.py file that contains two functions. \n",
    "The first one takes a tweet and preprocess it by doing cleaning, tokenization and stemming.\n",
    "The second one builds a dictionary with the keys are a tuple of (word, label) and the values are the count of this tuple in the dataset.\n",
    "\n",
    "It is preferred that you open this file and understand these functions as we will use them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047c87ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11337\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083b1597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148f0f8",
   "metadata": {},
   "source": [
    "# Requirement 1: Logistic Regression\n",
    "To predict the sentiment using logistic Regression, we need a way to transform the tweet to numberic features to be able to do the matrix multiplication of logistic regression.\n",
    "\n",
    "## Feature Extraction\n",
    "We will extract two features from the tweets:\n",
    "1. The first feature is the number of positive words in a tweet\n",
    "2. The second feature is the number of negative words in a tweet\n",
    "\n",
    "### extract_features function\n",
    "This function takes a tweet then preprocess it to get the words the it should use the freqs dictionary to calculate the positive feature and the negative feature. If a word is positive and its count in the freqs dictionary is 50 then the tweet positive feature should be increased by 50. If a word doesn't exist in the freqs dictionary then you can consider the count as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0395509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: the text of a tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,2)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    precessed_tweet = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 2)) \n",
    "    \n",
    "    ############################## TODO: Calculate positive and negative features ##################################\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in precessed_tweet:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0, 0] += freqs.get((word, 1), 0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0, 1] += freqs.get((word, 0), 0)\n",
    "        \n",
    "    #################################################################################################################\n",
    "    assert(x.shape == (1, 2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8b8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Your function\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "\n",
    "assert (tmp1 - np.array([[3020,   61]])).sum() == 0, \"Feature Extraction Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55a0b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_tweets_to_features(tweets, freqs):\n",
    "    \"\"\"\n",
    "    This function takes the tweets as strings and extracts the features for every tweet\n",
    "    \n",
    "    Input: \n",
    "    - tweets: list of strings (tweets)\n",
    "    - freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    \n",
    "    Returns:\n",
    "    - X: numpy array of shape (len(tweets), 2) \n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.zeros((len(tweets), 2))\n",
    "    \n",
    "    ################################### TODO: calculate each tweet feature vector and store it in X ###################\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        X[i, :] = extract_features(tweet, freqs)\n",
    "    \n",
    "    ###################################################################################################################\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b6572c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n",
      "(8000, 1)\n"
     ]
    }
   ],
   "source": [
    "X = input_tweets_to_features(train_x, freqs)\n",
    "Y = train_y\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e99191",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "In this part, we will use sklearn logistic regression model to train and test our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ebbd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c66c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(X, Y):\n",
    "    \"\"\"\n",
    "    This function trains logistic regression model\n",
    "    \n",
    "    Inputs:\n",
    "    - X: training data\n",
    "    - Y: labels\n",
    "    \n",
    "    Returns:\n",
    "    - logistic regression model after training\n",
    "    \"\"\"\n",
    "    \n",
    "    lr = LogisticRegression(random_state=5)\n",
    "    \n",
    "    ################################# TODO: train the lr model (hint: check fit from sklearn) #####################\n",
    "    lr.fit(X, Y)\n",
    "    \n",
    "    ###############################################################################################################\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a5b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = train_lr(X, Y.reshape(len(Y),))\n",
    "\n",
    "assert (lr.coef_ - np.array([[0.00903432, -0.01027023]])).sum() < 1e-6, \"Training Error\"\n",
    "assert lr.intercept_[0] - 0.99980174 < 1e-7, \"Training Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a4df3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, X):\n",
    "    \"\"\"\n",
    "    This function takes a classification model and input features to predict their labels\n",
    "    \n",
    "    Inputs:\n",
    "    - clf: classifier trained by sklearn\n",
    "    - X: input matrix of shape (TweetsDataCount, NFeatures)\n",
    "    \n",
    "    Returns:\n",
    "    - Y_pred: prediction matrix of shape(TweetsDataCount,)\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = None\n",
    "    ######################### TODO: predict labels (hint: check predict function from sklearn) ############################\n",
    "    Y_pred = clf.predict(X)\n",
    "    \n",
    "    #######################################################################################################################\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57be75fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test = input_tweets_to_features(test_x, freqs)\n",
    "Y_test = test_y\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc79b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = predict(lr, X_test)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51676e4e",
   "metadata": {},
   "source": [
    "## Let's test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baf00352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1000\n",
      "         1.0       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79750f",
   "metadata": {},
   "source": [
    "# Requirement 2: Naive Bayes\n",
    "As you know, naive bayes is based on words frequencies. To train a Naive Bayes classifier we need to do the following:\n",
    "\n",
    "## 1. Bag of Words:\n",
    "We need to represent each tweet with a vector of size V where V is the size of vocabulary and each entry represents the count of the word at this index. For this we will use the count vectorizer of sklearn\n",
    "\n",
    "## 2. Train Naive Bayes:\n",
    "Training the Multinomial Naive Bayes needs the matrix that represents the documents as bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36620eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_CountVectorizer(corpus):\n",
    "    \"\"\"\n",
    "    This function takes list of documents and preprocess them for CountVectorizer\n",
    "    \n",
    "    Inputs:\n",
    "    - corpus: List of strings\n",
    "    \n",
    "    Returns:\n",
    "    - processed_corpus: List of strings\n",
    "    \"\"\"\n",
    "    processed_corpus = None\n",
    "    ################################# TODO: implement the following steps ####################################\n",
    "    \n",
    "    # preprocess tweets (you should have a list of lists since you will call process_tweet on all tweets)\n",
    "    processed_tweets = [process_tweet(tweet) for tweet in corpus]\n",
    "    \n",
    "    # append the tokens of each tweet together seperating them by white space\n",
    "    # As the CountVectorizer needs list of strings\n",
    "    # hint: check str.join()\n",
    "    processed_corpus = [' '.join(tokens) for tokens in processed_tweets]\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "072876c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_processed_train_x = preprocess_for_CountVectorizer(train_x)\n",
    "NB_processed_test_x = preprocess_for_CountVectorizer(test_x)\n",
    "\n",
    "assert len(train_x) == len(NB_processed_train_x), \"Processing Error\"\n",
    "assert len(test_x) == len(NB_processed_test_x), \"Processing Error\"\n",
    "assert NB_processed_train_x[0] == 'followfriday top engag member commun week :)', \"Processing Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "483964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_count_vectorizer(processed_train_corpus):\n",
    "    \"\"\"\n",
    "    This function takes processed training corpus and trains a CountVectorizer\n",
    "    \n",
    "    Inputs:\n",
    "    - processed_train_corpus: list of tweets\n",
    "    \n",
    "    Returns:\n",
    "    - vectorizer: CountVectorizer Object after training\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = None\n",
    "    ################################### TODO: Create and Fit the vectorizer ##################################\n",
    "    # Create the Vectorizer\n",
    "    # hint1: check CountVectorizer from sklearn\n",
    "    # hint2: You will need to specify the token_pattern parameter as the default one will miss some tokens\n",
    "    vectorizer = CountVectorizer(token_pattern=r'([\\S]+)')\n",
    "    \n",
    "    # fit the vectorizer\n",
    "    vectorizer.fit(processed_train_corpus)\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "839ec8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BoWVectorizer = train_count_vectorizer(NB_processed_train_x)\n",
    "train_x_BOW = BoWVectorizer.transform(NB_processed_train_x)\n",
    "test_x_BOW = BoWVectorizer.transform(NB_processed_test_x)\n",
    "assert len(BoWVectorizer.vocabulary_) == 9083, \"Count Vectorizer Error\"\n",
    "assert train_x_BOW.shape == (8000, 9083), \"Count Vectorizer Error\"\n",
    "assert test_x_BOW.shape == (2000, 9083), \"Count Vectorizer Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "983b0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf39648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB(X, Y):\n",
    "    \"\"\"\n",
    "    This function takes the document frequency matrix (BoW matrix) and trains a MultinomialNB\n",
    "    \n",
    "    Inputs:\n",
    "    - X: Document frequency matrix\n",
    "    - Y: the labels\n",
    "    Returns:\n",
    "    - MultinomialNB Classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    NB = None\n",
    "    ############################## TODO: create and train Add-one Smoothed NB Classifer #############################\n",
    "    # Create the NB classifier (hint: check MultinomialNB sklearn documentation for add-one smoothnig)\n",
    "    NB = MultinomialNB(alpha=1.0)\n",
    "    \n",
    "    # train\n",
    "    NB.fit(X, Y)\n",
    "    \n",
    "    ##################################################################################################################\n",
    "    return NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d52a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = train_NB(train_x_BOW, train_y.reshape(len(train_y),))\n",
    "\n",
    "assert NB.classes_[1] == 1, \"NB Error\"\n",
    "assert NB.class_log_prior_[1] - -0.69314718 < 1e-8, \"NB Error\"\n",
    "assert NB.class_count_[1] == 4000, \"NB Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(NB, test_x_BOW)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f669459",
   "metadata": {},
   "source": [
    "# Let's test the NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
